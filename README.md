# Indroduction







## Important: Donâ€™t Confuse AI Depth with Physical Stereo

For NIR stereo tracking, **AI is not required**.

EdgeTrack is built on **deterministic, geometry-based NIR stereo vision** as the primary measurement system. Depth is computed from **physical baseline geometry, calibrated optics, and synchronized capture** â€” not from learned priors or dataset inference.

AI is **optional** and only used where it provides measurable benefit, such as:

* Stability monitoring
* Lightweight classification
* Left/right consistency checks
* Failure detection and recovery
* Semantic interpretation (gesture meaning, object type, intent)

The **core 3D reconstruction pipeline remains purely geometric and metric**.

### Why Multi-View Geometry Is Stronger Than AI Guessing

A single stereo rig can benefit from AI assistance in difficult cases.

However, the structurally stronger solution is **multi-view geometry**.

With **2â€“3 synchronized stereo rigs**, the system gains:

* Reduced occlusions
* Redundant triangulation
* Cross-validation between viewpoints
* Increased robustness without relying on learned priors

In many real-world setups, this reduces the need for AI almost entirely.

Geometry scales predictably.
Inference does not.

### Example: Appleâ€™s Depth Pro

Appleâ€™s Depth Pro is a powerful **monocular AI depth model**.

It is technically impressive â€” but it does not replace physical stereo measurement.

* AI depth infers structure from statistical patterns learned during training.
* Stereo depth measures disparity derived from real-world geometry and baseline separation.

Both approaches are valid â€” but they serve different purposes.

In EdgeTrack, AI models act as **assistive layers**, not as the measurement foundation.

### Physical Stereo vs Neural Stereo vs AI Depth

| Property                   | Classic Stereo (Geometry)       | Neural Stereo (AI-assisted matching) | Monocular AI Depth                |
| -------------------------- | ------------------------------- | ------------------------------------ | --------------------------------- |
| **Number of cameras**      | 2                               | 2                                    | 1                                 |
| **Requires baseline**      | Yes                             | Yes                                  | No                                |
| **Depth principle**        | Triangulation from disparity    | Learned disparity estimation         | Learned depth inference           |
| **Metric scale accuracy**  | True metric (after calibration) | True metric (after calibration)      | Often relative unless constrained |
| **Determinism**            | High (repeatable geometry)      | Medium (model-dependent)             | Low (probabilistic inference)     |
| **Training required**      | No                              | Yes                                  | Yes                               |
| **NPU/GPU requirement**    | No                              | Often beneficial                     | Required for real-time            |
| **Per-frame latency**      | Low, predictable                | Medium to high                       | Medium to high                    |
| **Multi-view consistency** | Naturally consistent            | Needs fusion logic                   | Needs full reconstruction logic   |
| **Texture-poor surfaces**  | Can struggle without pattern *  | Often improved                       | Often improved                    |
| **Gloss / reflections**    | Controlled with NIR + filtering | Model-dependent                      | Can hallucinate                   |
| **Occlusion handling**     | Solved via multi-view geometry  | Improved with fusion                 | Must infer hidden geometry        |
| **Interpretability**       | Physical measurement            | Hybrid                               | Statistical estimate              |

\* In controlled indoor setups (no direct sunlight, minimal glare, stable NIR illumination), stereo performs extremely well.
Failure modes typically appear on:

* Very low-texture materials
* Strong specular reflections
* Severe occlusions

EdgeTrack addresses these primarily through **optics and geometry**, not neural compensation.

The current prototype uses **diffuse (matte) NIR illumination** to generate a clean, homogeneous flood field. This improves stereo correspondence and reduces hotspots that destabilize matching.

On top of that, **MultiView coverage (2â€“3 rigs)** significantly increases robustness by:

* Reducing occlusions
* Increasing usable perspective diversity
* Improving triangulation reliability

Structured-light projection can improve extreme texture-poor scenes. However, it adds cost, optical complexity, synchronization constraints, and cross-talk risks â€” often without clear benefit for EdgeTrackâ€™s primary use case.

For **close-range hand interaction (0.5â€“0.8 m)**, natural micro-texture of skin and clothing combined with controlled NIR lighting is typically sufficient. Structured light is therefore unnecessary in most scenarios.

### Architectural Positioning

EdgeTrack follows a strict hierarchy:

1. **Primary layer:** Deterministic NIR stereo geometry
2. **Secondary layer (optional):** AI-based refinement or validation
3. **Semantic layer:** Gesture interpretation and high-level understanding

This avoids a common market confusion:

> AI depth estimation is not equivalent to physical stereo measurement.

Stereo **measures**.
Neural stereo **optimizes matching**.
Monocular AI **estimates**.

When combined properly, they complement each other â€” but they are not interchangeable.

---

## Comparison of Available Hardware on the Market

> Note: This comparison focuses on architectural capabilities and integration models.
> Feature availability may vary by configuration and firmware.

| Feature / Focus                              | ZED 2i & RealSense |   Bumblebee   |   Leap Motion    | OptiTrack | Basler Stereo |    Orbbec     |     EdgeTrack    |
|----------------------------------------------|:------------------:|:-------------:|:----------------:|:---------:|:-------------:|:-------------:|:----------------:|
| Primary use case                             | Depth sensing / XR | Stereo vision | XR hand tracking |   MoCap   | Stereo vision | Depth sensing | Editor authoring |
| Capture FPS (typical)                        |         Mid        |      Mid      |       High       | Very High |      Low      |      Mid      |    Very High*    |
| Stereo / multi-camera                        |         ğŸŸ¢         |       ğŸŸ¢     |        ğŸŸ¡         |    ğŸŸ¢     |      ğŸŸ¢      |       ğŸŸ¢      |        ğŸŸ¢       |
| RAW10 or RAW12                               |         ğŸ”´         |       ğŸŸ¢     |        ğŸ”´         |    ğŸŸ¡     |      ğŸŸ¢      |       ğŸ”´      |        ğŸŸ¢       |
| RAW10 ingest on the edge (CPU/GPU)           |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| Native multi-device fusion                   |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸŸ¢     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| Phase-offset capture (TDM Module)            |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| **Deterministic event layer**                |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |  **ğŸŸ¢**   |      ğŸ”´      |       ğŸ”´      |      **ğŸŸ¢**     |
| **Editor-oriented API**                      |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |      **ğŸŸ¢**     |
| Open-source core                             |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| Edge-side processing (on-device)             |         ğŸŸ¢         |       ğŸŸ¢     |        ğŸŸ¢         |    ğŸ”´     |      ğŸ”´      |       ğŸŸ¢      |        ğŸŸ¢       |
| Linux-based edge device (on-board OS)        |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| AI On-device accelerator support (NPU/GPU)   |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢**     |
| Expandable hardware (add-ons / upgrades)     |         ğŸ”´         |       ğŸ”´     |        ğŸ”´         |    ğŸ”´     |      ğŸ”´      |       ğŸ”´      |        ğŸŸ¢       |
| Depth range (typical)                        |      ~0.5â€“6 m      |    ~0.3â€“5 m  |     ~0.1â€“1 m      | ~0.2â€“20 m |  ~0.2â€“1.0 m  |   ~0.15â€“10 m  |  0.1â€“10 m***  |
| Depth resolution @ 0.2 m                     |       ~2 mm        |     ~2 mm    |      ~0.5 mm      | ~<0.2 mm  |    ~0.04 mm  |    ~1 mm      |  ~0.2 mm***    |
| Depth resolution @ 0.5 m                     |       ~5 mm        |     ~5 mm    |      ~2 mm        |  ~<0.5 mm |    ~0.5 mm   |    ~4 mm      |  ~1.5 mm***    |
| Depth resolution @ 1.2 m                     |      ~15 mm        |     ~15 mm   |         -         |   ~2 mm   |     ~2 mm    |    ~15 mm     |  ~6 mm***      |
| Typical interface                            |         USB        |       USB     |        USB       | Ethernet  |  USB / GigE  |      USB      |   Ethernet      |
| Typical price range                          |         $$$$       |      $$$$$    |        $$        |   $$$$$$  |     $$$$$    |      $$$      |       $$$       |


### Footnotes

\* Capture rates depend on camera selection and edge platform configuration.
Effective update rates above 1000 Hz are achieved via **TDM phase-offset interleaving** across multiple synchronized stereo rigs (a *virtual/effective* update rate), not from a single physical camera.

\** Accelerator support depends on the selected edge platform (optional NPU/GPU modules).

\*** EdgeTrack is primarily optimized for **high-precision operation up to ~1.2 m** using homogeneous NIR flood illumination.

For extended range scenarios (up to ~10 m), multiple configurations are possible:

* Integrated **VCSEL dot-pattern modules** (active stereo assist)
* Neural stereo without strict bandpass filtering (environment-dependent)
* External high-power NIR flood panels

Range and resolution depend on baseline, optical configuration, illumination mode, and edge processing setup.

### Notes on depth range & â€œresolutionâ€ figures

The depth range and resolution values in this table are **order-of-magnitude estimates** meant for architectural comparison.  
In real systems, results depend heavily on optics (FOV, focus, IR filtering), surface properties, illumination power/pattern, exposure, and matching/denoise pipelines.

#### OptiTrack (marker-based triangulation)
OptiTrack is **not a stereo depth camera**. It reconstructs 3D positions by **triangulating reflective markers** across a calibrated camera array.
With a well-designed volume (good geometry, calibration, lens choice, controlled lighting), **sub-millimeter to millimeter accuracy** is achievable.
Because the measurement principle is different from active depth cameras (stereo/ToF/structured light), the â€œdepth resolutionâ€ numbers are **not directly comparable**.

**OptiTrack** achieves determinism through a **centralized camera array** and **proprietary synchronization hardware**. While OptiTrack offers a **â€œraw grayscaleâ€** video mode, this is primarily a **Reference Mode** for monitoring/aiming rather than a stream intended for **3D reconstruction**; it is also **not fully synchronized** and typically runs at a **lower frame rate**. **EdgeTrack**, in contrast, targets determinism **at the edge** via **distributed TDM phase-offset capture** and prioritizes **uncompressed RAW10/RAW12 sensor streams** (instead of H.264/H.265 pipelines) to preserve pixel fidelity for **stable, reproducible stereo reconstruction**.

#### Basler Stereo (industrial stereo)
Baslerâ€™s industrial stereo solutions can achieve **very high precision in the near field** (e.g., ~0.04 mm at ~0.2 m under optimal conditions).
However, **maximum precision typically comes with trade-offs**: reduced effective FPS at full depth quality, strict calibration requirements, and controlled illumination/scene texture.  
This is why the table marks Basler as **â€œLowâ€ FPS (typical)** in a conservative, market-wide comparison.

#### RealSense / ZED / Orbbec (consumer/prosumer depth cameras)
Many depth-camera vendors specify accuracy as a **percentage of distance** (a common rule-of-thumb is ~1â€“2% of range, depending on model and conditions).
For that reason, the mm-values shown here are presented as **realistic ranges**, not best-case lab numbers, and should be interpreted as â€œwhat you can typically expectâ€ rather than guaranteed performance.

#### Leap Motion
Leap Motion uses a dual-camera hardware setup; however, the system does not expose or process stereo data as a general-purpose stereo vision pipeline.


---

## âš™ï¸ VPU vs CPU (Stereo Disparity)

**VPU-based systems** shine when you want **efficient, ready-to-use dense depth** (e.g., 720p @ 30 FPS) with low host load.
**CPU / RAW-first systems** shine when you need **maximum control**, **deterministic timing**, **ROI instead of full-frame**, and **multi-rig synchronization**.

Thatâ€™s why EdgeTrack focuses on **CPU + RAW-first control**: my priority is **precise geometry, timing consistency, and modular multi-rig operation** â€” not â€œdepth everywhere at any costâ€.

| Feature                                 | ğŸ–¥ï¸ CPU (Pi 5) | ğŸ–¥ï¸ CPU (Threadripper) | ğŸ“Ÿ VPU (On-device depth) |
|------------------------------------------|:-------------:|:----------------------:|:------------------------:|
| ğŸ—ºï¸ Dense depth efficiency               | âš ï¸ Medium (CPU-limited) | ğŸš€ Very High (brute-force compute) | âœ… High (hardware-accelerated) |
| ğŸ¥ 720p @ 30 FPS (dense)                | âš ï¸ Borderline (depends on disparity range) | âœ… Stable | âœ… Stable |
| âš¡ 120 FPS (dense)                      | âŒ Not practical | âš ï¸ Possible (heavy power usage) | âŒ Not typical |
| ğŸ¯ ROI matching (targeted processing)   | âœ… Very strong | âœ… Very strong | âš ï¸ Limited |
| ğŸï¸ RAW control / pipeline freedom      | âœ… Full control | âœ… Full control | âš ï¸ Limited |
| ğŸ”— Multi-rig sync / deterministic phase | âœ… Ideal | âš ï¸ Complex (host-dependent) | âš ï¸ Limited (device/framework dependent) ||

**In short:**
ğŸ“Ÿ VPU is great for **â€œeasy dense depth output.â€**
ğŸ–¥ï¸ CPU/RAW-first is great for **â€œcontrolled, repeatable geometryâ€** and **multi-rig workflows.**

---

## Why RAW Stereo Capture Instead of H.265

Most commercially available stereo and depth cameras rely on **H.264/H.265 video compression** to reduce bandwidth and simplify integration. While this approach is sufficient for visualization, XR previews, and general perception tasks, it introduces **lossy compression artifacts, temporal smoothing, and non-deterministic frame behavior** that negatively affect precise 3D reconstruction.

For high-accuracy stereo vision, **uncompressed RAW sensor data** is fundamentally superior. Using **RAW10** preserves the original linear pixel intensities produced by the image sensor, without spatial or temporal loss. This enables more robust stereo matching, accurate sub-pixel disparity estimation, and consistent depth reconstruction across frames.

In addition, RAW capture allows precise control over **exposure, gain, and synchronization**, which is essential for deterministic multi-camera systems. When combined with **near-infrared (NIR) illumination**, RAW stereo becomes significantly more stable under varying lighting conditions and supports reliable operation in controlled and industrial environments.

For these reasons, the project deliberately avoids H.265-based pipelines and focuses on **RAW10 stereo capture**, prioritizing determinism, precision, and authoring-grade 3D data quality over bandwidth efficiency.

> Note: H.265 is common for AI segmentation streams, but compression artifacts can bias models. In EdgeTrack, **geometry is the baseline**; AI is optional.

---

## Interface: USB vs. Ethernet vs. WLAN

USB, Ethernet, and WLAN are all commonly used to connect cameras and tracking devices, but they differ significantly in terms of determinism, scalability, and reliability.

**USB** is widely available and easy to set up, making it well suited for single-device configurations, prototyping, and consumer peripherals. However, USB is typically host-driven and shared across multiple devices on the same controller. As a result, bandwidth contention, variable latency, and timing jitter can occur, especially in multi-camera setups or under high system load. While acceptable for many use cases, these characteristics can limit predictability in time-critical pipelines.

**Ethernet** is designed for distributed and scalable systems. Each device operates independently on the network and commuates using explicit packetization, buffering, and timestamps. This enables more predictable latency, cleaner synchronization across multiple devices, and stable performance over longer cable distances. Ethernet also supports structured topologies using switches, VLANs, and Power-over-Ethernet (PoE), making it well suited for multi-rig and multi-room setups. For professional capture and deterministic processing pipelines, Ethernet is often the preferred transport layer.

**WLAN (Wi-Fi)** provides flexibility and mobility by removing physical cables, which can be advantageous in portable or rapidly reconfigurable environments. However, wireless links are inherently subject to interference, variable airtime, and changing network conditions. These factors can introduce fluctuating latency, packet loss, and jitter, which complicate synchronization and reproducibility. While modern Wi-Fi standards offer high peak bandwidth, sustained real-time performance is harder to guarantee.

In practice, USB is convenient for simple setups, Ethernet offers the most control and scalability for deterministic systems, and WLAN trades predictability for mobility and ease of deployment.

> **Note:** For tight timing, direct NIC connections are preferred. Switches usually add small latency, but can add variability under congestion; use QoS/VLAN/PTP if deterministic timing is required.

---

## Latency Comparison (Typical, No Aggressive Tuning)

| # | Class                                                         | Typical latency (no tuning)                                          | Jitter / determinism                            | Main latency contributors                                                                       | Practical notes                                                                                                                                                                                                                                     |
| - | ------------------------------------------------------------- | -------------------------------------------------------------------- | ----------------------------------------------- | ----------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| 1 | **High-end USB webcam (UVC, often ISP + MJPEG/H.264)**        | **50â€“200 ms** (often â€œnoticeably laggyâ€)                             | **poorâ€“medium**, highly variable                | Camera ISP + **multi-frame buffering** + (optional) codec/decode + host color/format conversion | Often â€œbeautifiedâ€ internally (AE/NR/scaling) and buffered for smooth video. Usually not suitable for low-latency VR/tracking workloads.                                                                                                            |
| 2 | **Industrial USB (USB3 Vision, often uncompressed/RAW/mono)** | **10â€“30 ms**                                                         | **medium**                                      | Sensor (exposure/readout) + USB transfer + host queue/copy                                      | Much better than webcams. **Bottlenecks:** USB bandwidth per host controller, cable/EMI constraints, and multi-camera setups saturate quickly. Multi-view can heavily load the host (DMA + CPU/GPU processing).                                     |
| 3 | **Industrial GigE (GigE Vision, UDP)**                        | **15â€“40 ms**                                                         | **medium**, jitter possible                     | Packetization + NIC/kernel/queueing + (optional) switch/network effects                         | Robust and flexible (long cables, PoE options). Without tuning you can see **jitter/spikes** (network contention, driver scheduling, interrupts). Multi-camera works, but host + network design becomes critical.                                   |
| 4 | **CoaXPress (frame grabber)**                                 | **5â€“20 ms**                                                          | **goodâ€“very good**                              | Sensor dominates; transport is almost â€œinvisibleâ€                                               | Highly deterministic with very fast triggering. **Downside:** **expensive** (cameras + frame grabber + cables) and a heavier industrial ecosystem. Multi-camera scales well, but cost/integration is high.                                          |
| 5 | **EdgeTrack (edge compute + results over Ethernet only)**     | **5â€“15 ms (keypoints/ROI)** / **10â€“30 ms (dense depth/point cloud)** | **very good**, deterministic (HW trigger + MCU) | Sensor + edge compute time (minimal transport overhead)                                         | You â€œspendâ€ latency on **edge compute**, but save a lot elsewhere: no video codec, no RAW transport, and far less host load. Multi-view scales better because the host fuses **small result streams** (CoreFusion) instead of handling full frames. |

### Why #5 often â€œfeelsâ€ best in real systems

* For **#1â€“#4**, the host typically has to **move frames + decode + convert formats + run vision**, which adds latency, jitter, and CPU/GPU load.
* For **#5**, the expensive work happens **before transport**, and you transmit only **keypoints/ROI/point clouds**, leading to lower, more predictable latency and a simpler host pipeline.

---
































## My Architecture and Design â€” Unconventional and Beyond the Market

### 1. Clear Separation of Capture and Processing

#### Problem

Native **MIPI CSI** camera interfaces offer excellent bandwidth, low latency, and precise timing, but they suffer from a major limitation: **very short cable lengths**, which makes larger or distributed camera setups impractical.

**USB-based** camera solutions allow longer cables, but USB is **interrupt-driven and host-dependent**, which typically results in **higher latency, increased jitter, and less deterministic timing**â€”especially problematic for tightly synchronized multi-camera systems.

**Ethernet/LAN-based** cameras improve distance and deployment flexibility, but many implementations still rely on **OS-level interrupts, buffering, and packet scheduling**, which are not inherently optimized for **hard real-time capture** or frame-accurate multi-camera synchronization. In practice, achieving truly deterministic behavior often requires a **real-timeâ€“tuned OS and network stack**, along with careful system-level optimization.

In addition, purpose-built **GigE / 2.5GigE machine-vision cameras** remain relatively expensiveâ€”often **â‚¬500+ per unit**â€”which makes large multi-camera arrays **cost-heavy** and limits scalability from a hardware-budget perspective.

At the high end, **CoaXPress** can deliver outstanding performance with direct, low-latency data paths into CPU/GPU memory. However, it comes with **very high hardware cost**, requires dedicated **frame grabbers**, and scales poorly **in terms of system cost and integration complexity**. Beyond the capture hardware itself, processing **multiple high-resolution cameras on a single host** can place a substantial load on the CPU/GPUâ€”often pushing systems toward high-end workstation-class hardware (e.g., Threadripper-class systems) and significant engineering effort to optimize the processing pipeline.

#### Solution

EdgeTrack separates **image capture** from **high-level processing** by moving **reconstruction and preprocessing directly to the edge**. Instead of concentrating the entire workload on a single, expensive host system, each edge device performs its **local reconstruction tasks** using native MIPI CSIâ€”where it performs bestâ€”and exports only **processed, compact 3D data** (e.g., keypoints, tool poses, sparse geometry) over the network.

This architecture preserves the **timing fidelity and signal quality** of native CSI capture while remaining **cost-efficient, scalable, and deployment-friendly**.

---

### 2. Ethernet-Native Alternative to CoaXPress

A **CoaXPress-based** camera infrastructure is a high-end solution in terms of **bandwidth and timing precision**, but it is **cost-intensive** and requires substantial **integration effort**, including dedicated **frame grabbers** and complex host-side pipelines.

Instead, comparable practical precision **for the target outputs** can be achieved through a combination of:

- **Well-defined multi-view geometry**
- **Calibrated stereo triangulation**
- **Edge-side preprocessing**
- **Early fusion in CoreFusion**

By transmitting **stable 3D primitives** (e.g., keypoints, tool poses, sparse geometry) rather than raw video streams, the system delivers **reproducible, low-jitter 3D signals** with significantly lower bandwidth requirements and reduced host-side complexity.

For the intended application, this architecture can **approach CoaXPress-class results** for **pose/keypoint accuracy and temporal stability**, while offering:

- **Simpler integration**
- **Lower hardware and maintenance costs**
- **Much better scalability**

Additional rigs can be added via **standard LAN connections**, rather than consuming limited frame-grabber channels and centralized capture resources.

---

### 3. TDM Phase-Offset Capture for Deterministic Timing

EdgeTrack uses **phase-offset global-shutter capture via Time-Division Multiplexing (TDM)**.
Instead of exposing all cameras simultaneously, multiple stereo rigs are triggered in **time-interleaved phases**.

This design:

* **Reduces occlusion**
* Improves **temporal consistency**
* Enables more **stable, repeatable input**

â€”especially important in **close-range, tool-centric workflows** where precision matters more than visual realism.

EdgeTrack is **markerless by default**, but supports **optional, minimal markers** when additional robustness is required.
Examples include subtle fingertip markers or markers placed directly on a tool. A â€œ3D pencilâ€ assisted by two small markers can provide **pen-like precision**, enabling reliable writing gestures or even **virtual keyboard interaction**.

A small **MCU-based trigger controller** generates deterministic, phase-shifted triggers for **up to eight stereo rigs** at **120 FPS per rig**.
When fused in **CoreFusion**, this results in an **effective aggregate update rate of up to ~960 Hz**, while maintaining **low jitter and high temporal stability**, depending on configuration and synchronization.


#### Why TDM Is Not Distributed Over LAN (and Why an MCU Can Still Make Sense)

A dedicated MCU (e.g., **RP2040**) is **not strictly required**â€”on a **Raspberry Pi 5**, TDM trigger signals can be generated locally using **hardware timers and/or DMA-driven GPIO** with sufficient precision for **120 FPS**.

The key distinction is **where timing is generated**:

* **Ethernet/LAN is excellent for data transport** (payload streaming, timestamps, configuration/control messages).
* But it is **not ideal as a real-time trigger bus**, because packet delivery depends on **OS scheduling, buffering, interrupts/NAPI, NIC behavior, and switch latency**, which introduces **variable jitter**.

Therefore, EdgeTrack uses **LAN for payload and timestamps**, while **TDM phase triggering is generated locally on each edge device** (Pi-side or MCU-side). If multiple edge devices must share a common phase reference, synchronization is handled via a **deterministic wired sync bus** (e.g., **RS-485**) or a **shared time base** (e.g., clock sync + scheduled start times), rather than sending **per-frame triggers** over the network.

> In short: **LAN transports data; the edge generates timing.**

### 4. Short Features

#### Stereo-First, Not AI-First

EdgeTrack uses **NIR stereo vision** as the primary tracking method. Depth is computed through **triangulation**, which means the system measures real geometry instead of guessing it. This makes the output predictable, repeatable, and easier to validateâ€”especially important in professional workflows where stability matters more than â€œimpressive demos.â€

#### Designed for Short-Exposure Motion Freeze

The front-facing NIR illumination is not decorative. It enables **ultra-short exposure times** that freeze fast motion and reduce blur. This improves stereo correspondence, reduces noise, and stabilizes tracking under real-world movementâ€”something many consumer-grade depth solutions struggle with.

#### MultiView as the Main Upgrade Path

Instead of relying on heavier models to â€œfixâ€ failures, EdgeTrack scales through **MultiView geometry**. With **2â€“3 stereo rigs**, occlusions are reduced and robustness increases dramatically. In many scenarios, MultiView delivers reliability that would otherwise require complex AI, but without losing determinism.

#### Edge Processing, Minimal Data Output

EdgeTrack is designed to process data on the edge and export only what matters:

* 3D keypoints
* ROI point clouds
* compact geometric results

This reduces bandwidth, lowers latency, and keeps the system scalable for multi-sensor setups.

#### Optional AI as a Support Layer

AI can be added where it truly helpsâ€”such as plausibility checks, lightweight classification, or recovery in difficult edge cases. But AI is not the core. The core is a system you can measure, tune, and trust.

---



































## LiDAR and ToF

LiDAR and Time-of-Flight (ToF) sensors are frequently used for 3D perception, but they introduce trade-offs that can be problematic for **high-precision, close-range authoring and interaction** workflows.

**Key limitations in this context:**

* **Effective spatial resolution at close range:** Many ToF/LiDAR modulesâ€”especially compact or consumer-class devicesâ€”provide lower effective spatial detail than multi-view stereo, which limits fine hand, finger, or tool-level tracking.
* **Unstructured data for interaction:** Dense depth/point outputs are not automatically useful for control. Interaction benefits most from **stable structure** (keypoints, edges, marker geometry, ROI constraints) rather than uniform depth everywhere.
* **Temporal noise and instability:** Depth measurements can be affected by multi-path interference, surface reflectivity, ambient IR, and sensor-internal filtering, causing depth flutter and jitterâ€”undesirable for repeatable input.
* **Multi-view scaling complexity:** Scaling active depth sensors across multiple viewpoints can increase cost and complexity due to emitter interference management (time-multiplexing, frequency/coding separation) and synchronization constraints.
* **Limited deterministic control:** Many modules behave as closed systems with fixed timing and internal depth processing, offering limited external synchronization and reduced transparency for frame-accurate multi-sensor fusion.
* **Integration and cost overhead:** High-quality LiDAR/industrial ToF systems with low noise and good stability are often expensive and may require proprietary SDKs and constrained deployment workflows.
* **Near-field suitability:** Many LiDAR/ToF systems are optimized for mid-to-long range. For desktop-scale workspaces, **stereo vision with controlled illumination** often provides higher effective precision and better repeatability.

**Short:** LiDAR/ToF sensors typically increase BOM, calibration, and integration complexity. In contrast, a simple global-shutter mono camera is easier to manufacture and scale.

**EdgeTrack design choice:** EdgeTrack deliberately favors synchronized global-shutter stereo vision with controlled NIR illumination, enabling:

* higher spatial detail at close range
* deterministic timing via external strobe and phase control
* better scalability across multiple rigs
* lower system and integration cost
* full control over the capture and reconstruction pipeline

For deterministic 3D authoring, precise hand/tool interaction, and reproducible editor workflows, multi-view stereo with explicit timing control is often a more accurate, scalable, and transparent foundation than LiDAR/ToF-based systems.

---

















## â­ Pixel format â€“ preference

| Format                     |   Rating   | Comment                                                                                  |
| -------------------------- | ---------- | ---------------------------------------------------------------------------------------- |
| **MJPEG / JPEG**           | â­â˜†â˜†â˜†â˜†     | Only for preview/debug. Strong artifacts, variable bitrate, poor for precise 3D.         |
| **YUV / YUYV / NV12**      | â­â­â˜†â˜†â˜†    | OK if you only use the **Y (luma)** channel. Extra bandwidth wasted on color info.       |
| **RAW8 / Y8 (8-bit mono)** | â­â­â­â˜†â˜†   | Solid baseline. Lower dynamic range, but good enough with proper NIR illumination.       |
| **RAW10**                  | â­â­â­â­â˜†  | Very good: higher dynamic range, finer quantization, still manageable bandwidth.         |
| **RAW12**                  | â­â­â­â­â­ | Ideal for high precision: maximum dynamic range and depth resolution, highest bandwidth. |

---

## âš™ï¸ Quick Engineering Comparison â€” What is the best interface for deterministic vision?

When designing a machine-vision or stereo system, the choice of sensor interface has a strong impact on latency, control, and system complexity.
Below is a simplified engineering comparison:

| Interface              | Additional Chips / Infra | RAW Access | Latency     | Determinism |
| ---------------------- | ------------------------ | ---------- | ----------- | ---------- |
| **MIPI CSI-2**         | very few                 | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |
| **USB2 (typical UVC)** | medium                   | â­â˜†â˜†â˜†â˜†     | â­â­â˜†â˜†â˜†     | â­â˜†â˜†â˜†â˜†     |
| **USB3 (typical UVC)** | medium                   | â­â­â˜†â˜†â˜†    | â­â­â­â˜†â˜†   | â­â­â˜†â˜†â˜†     |
| **GigE / GigE Vision** | many                     | â­â­â­â­â˜†  | â­â­â­â˜†â˜†   | â­â­â­â­â˜†  |
| **CoaXPress**          | heavy (framegrabber)     | â­â­â­â­â­ | â­â­â­â­â­ | â­â­â­â­â­ |

### Summary

**MIPI CSI-2** is typically the best choice when deterministic timing, minimal latency, and direct RAW sensor access are required. The sensor is connected almost directly to the SoC, which reduces hidden processing stages and keeps the pipeline transparent.

**USB cameras** usually include additional ISP and bridge chips. They are convenient and plug-and-play, but often introduce internal processing and buffering that reduce determinism.

**GigE cameras** are powerful for industrial networking and long cable distances, but typically require more intermediate logic (FPGA/ASIC, packetization, buffering), which increases system complexity.

**CoaXPress** is a high-end industrial interface designed for very high bandwidth and deterministic transmission. It typically requires a dedicated frame grabber card on the host side and specialized hardware inside the camera. While it offers excellent throughput, low latency, and strong determinism, it significantly increases system cost, hardware complexity, and power requirements compared to embedded MIPI-based designs.

For edge-processing architectures focused on precise timing and reproducible results, **MIPI CSI-2 provides the most transparent and controllable capture path**.










